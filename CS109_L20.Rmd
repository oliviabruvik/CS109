---
title: 'CS109: Probability for Computer Scientists'
author: "Olivia Beyer Bruvik"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
  output_dir: "/Users/oliviabeyerbruvik/Documents/Stanford/Y1Q2/CS109/CS109_markdowns/reports"
subtitle: 'Lecture 20: Algorithmic Analysis'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lecture 20: Algorithmic Analysis {.tabset}

## Topics
- Lecture 18: CLT
- Lecture 16: notes

## Null Hypothesis, $H_0$
Null hypothesis: there is no difference between the two groups, so everyone is drawn from the same (universal) distribution. Any difference you observe is due to sampling error.

1. Group A Samples: $\mu_1 = 3.1$
2. Group B Samples: $\mu_2 = 2.4$

## Bootstraping
1. Combine group A, B --> Universal distribution
2. Repeat 1,000,000 times:
  a) Fake group A', fake group B'
  b) Calculate difference in means
  c) Is difference greater than difference in means of A and B (0.7)?

### Code
```{Python, eval = FALSE}
n_larger = 0
hypothetical_effect_sizes = []

for i in tqdm(range(ITERATIONS)):
  fake_a = resample(universal, len(pop1))
  fake_b = resample(universal, len(pop2))
  fake_effect = abs(np.mean(fake_a) - np.mean(fake_b)) ## normal distribution (CLT)
  if fake_effect > effect_size:
    n_larger += 1
  hypothetical_effect_sizes.append(fake_effect)
```

## Distributions of Mean Diffs under Null Hypothesis {.tabset}
Folded normal. 

$$
Pvalue = \frac{n_{larger}}{len(ITERATIONS)}
$$

Results of flipping a coin 20 times: 4 tails and 16 heads.

### Bayesian: 
\[
\begin{aligned}
\text{Let's use Laplace prior} \\
X \sim Beta(a=17, b=5)
\end{aligned}
\]

### Frequentist
Use bootstrap!

## Expectation {.tabset}
$$
E[X] = \sum_x x \times P(X=x)
$$
ie. X = time to complete the medical diagnosis problem (in seconds)

### Expectation of a Sum
Holds regardless of dependency between X_i's:
$$
E[X+Y] = E[X] + E[Y] \\
E[\sum_{i=1}^n X_i] = \sum_{i=1}^n E[X_i]
$$

### Expectation of a Function
Law of unconscious statistician:
$$
E[g(X)] = \sum_{x} g(x) \times P(X=x) \\
E[X^2] = \sum_x x^2 \times P(X=x)
$$

## Expectations of RVs {.tabset}
### Expectation of Bernoulli's
Let $E_1, E_2, ..., E_n$ be events with indicator RVs $X_i$.

- If event $E_i$ occurs, then $X_i = 1$, else $X_i = 0$
- Recall $E[X_i] = P(E_i)$

$$
E[X_i] = 0 \times (1-P(E_i)) + 1 \times P(E_i)
$$

### Expectation of the Binomial
Let $Y \sim Bin(n,p)$.

- n independent trials
- Let $X_i = 1$ if i-th trial is a success, 0 otherwise. 
- $X_i \sim Ber(p)$

\[
\begin{aligned}
E[Y] &= E[\sum_{i=1}^n X_i] \\
&= \sum_{i=1}^n E[X_i] \\
&= E[X_i] + E[X_2] + ... + E[X_n] \\
&= np
\end{aligned}
\]

### Expectation of the Negative Binomial
Let $Y \sim NegBin(r,p)$.

- Recall Y is number of trials until r "successes"
- Let $X_i = # of trials to get success after (i-1)st success
- $X_i \sim Geo(p)$

\[
\begin{aligned}
Y &= X_1 + X_2 + ... + X_r = \sum_{i=1}^r X_i \\
E[Y] &= E[\sum_{i=1}^r X_i] \\
&= \sum_{i=1}^r E[X_i] \\
&= E[X_i] + E[X_2] + ... + E[X_r] \\
&= \frac{r}{p}
\end{aligned}
\]

## Differential Privacy
Aim to provide means to maximize the accuracy of probabilistic queries while minimizing the probability of identifying its records. <br>

100 independent values X1 … X100 where Xi ~ Bern(p)
```{Python, eval = FALSE}
# Maximize accuracy, while preserving privacy.
def calculateYi(Xi):
  obfuscate = random() # random() returns True or False with equal likelihood
  if obfuscate:
    return indicator(random())
  else:
    return Xi
```

$$
E[Y_i] = P(Y_i = 1) = \frac{p}{2} + \frac{1}{4} \\
Let \ Z = \sum_{i=1}^{100} Y_i \\
E[Z] = E[\sum_{i=1}^{100} Y_i] = \sum_{i=1}^{100} E[Y_i] = \sum_{i=1}^{100} \frac{p}{2} + \frac{1}{4} = 50p + 25 \\
Z = 48 \Rightarrow p \approx \frac{Z-25}{50} = 0.48
$$

## Computer Cluster Utilization
Computer cluster with k servers

- Requests independently go to server i with probability $p_i$
- Let event $A_i$ = server i receives no requests: $P(A_i) = (1-p_i)^n$
- Let Bernoulli Bi be an indicator for Ai
- X = # of events $A_1, A_2, … A_k$ that occur: $X = \sum_{i=1}^k B_i$
- Y = # servers that receive ≥ 1 request = k – X
- E[Y] after first n requests?
- Since requests independent:

\[
\begin{aligned}
E[X] &= E[\sum_{i=1}^k B_i] = \sum_{i=1}^k E[B_i] = \sum_{i=1}^k P[A_i] = \sum_{i=1}^k (1-p_i)^n \\
E[Y] &= k-E[X] = k-\sum_{i=1}^k (1-p_i)^n
\end{aligned}
\]

## Hash tables

Consider a hash table with n buckets
- Each string equally likely to get hashed into any bucket
- Let X = # strings to hash until each bucket ≥ 1 string
- What is E[X]?
- Let Xi = # of trials to get success after i-th success
  + where “success” is hashing string to previously empty bucket
  + After i buckets have ≥ 1 string, probability of hashing a string to an empty bucket is $p = \frac{n – i}{n}$
  + $P(X_i = k) = \frac{n-i}{n} (\frac{i}{n})^{k-1}$
  + equivalently: $X_i \sim Geo(\frac{n-1}{n})$
  + $E[X_i] = \frac{1}{p} = \frac{n}{n-i}$
  + $E[X_i] = \frac{n}{n} + \frac{n}{n-1} + \frac{n}{n-2} + ... + \frac{n}{1} = n[\frac{n}{n} + \frac{n}{n-1} + \frac{n}{n-2} + ... + \frac{n}{1}] = O(nlogn)$






