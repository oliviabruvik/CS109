---
title: 'CS109: Probability for Computer Scientists'
author: "Olivia Beyer Bruvik"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
  output_dir: "/Users/oliviabeyerbruvik/Documents/Stanford/Y1Q2/CS109/CS109_markdowns/reports"
subtitle: 'Lecture 23: Naive Bayes'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lecture 23: Naive Bayes {.tabset}

## Topics

## MLE vs. MAP {.tabset}
Data: $x^{(1)}, ..., x^{(n)}$ <br>

### Maximum Likelihood Estimation
$$
\hat{\theta_{MLE}} = argmax f(x^{(1)}, ..., x^{(n)}|\theta) \\
= argmax (\sum_i log f(x^{(i)}|\theta))
$$

### Maximum A Posteriori
$$
\hat{\theta_{MAP}} = argmax f(\theta| x^{(1)}, ..., x^{(n)}) \\
= argmax (log(g(\theta)) + \sum_i^n log (f(x^{(i)}|\theta)))
$$

### Notation
$\theta$ is shorthand for the event: $\theta = \theta$ <br>
$x^{(i)}$ is shorthand for the event: $X^{(i)} = x^{(i)}$ <br>

$$
\hat{\theta_{MAP}} = argmax f(\theta = \theta | X^{(1)} = x^{(1)}, ..., X^{(n)} = x^{(n)}) \\
= argmax f(\theta | x^{(1)}, ..., x^{(n)}) \\
$$

## MAP for Bernoulli and Binomial
Beta(a,b) is a conjugate prior for the probability of success in Bernoulli and Binomial distributions. 

$$
f(x) = \frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}
$$

- Prior: 
  + Laplace prior: $Beta(a = 2, b = 2)$
  + Saw $a+b-2 = 2$ imaginary trials: $a-1 = 1$ successes, $b-1 = 1$ failures
- Experiment: Observe n+m new trials: n successes, m failures
- Posterior: $Beta(a+n, b+m)$ = $Beta(2+n, 2+m)$
- MAP: $p = \frac{a+n-1}{a+b+n+m-2} = \frac{n+1}{n+m+2}$

Mode: Value with highest frequency. Argmax of distribution. <br>

## Conjugate distributions
Map estimator:
$$
\hat{\theta_{MAP}} = argmax f(\theta | X_{1}, X_2,...,X_n)
$$

Distribution parameter and conjugate distributions
Conjugate: if 

- Beta:
  + Bernoulli, p
  + Binomial, p
- Dirichlet
  + Multinomial, $p_i$
- Gamma
  + Poisson, $\lambda$
  + Exponential, $\lambda$
  
## MAP and MLE for Multinomial {.tabset}
Each experiment has M possible outcomes. What is the likelihood of a particular count of each outcome?

Multinomial is parametrized by $p_i$: the likelihood of outcome i on any one experiment. <br>

Dice:

- M = 6
- $p_i = \frac{1}{6}$

### MLE for Multinomial
MLE estimate of the probability of outcome i is equal to the number of observed outcomes of type i divided by the number of observations. 
$$
p_i = \frac{n_i}{n}
$$

### MAP for Multinomial
#### With Laplace Prior
MAP estimate of the probability of outcome i is equal to the ration of the number of observed outcomes of type i + 1 to the sum of the number of observations and the number of outcome types. 

$$
p_i = \frac{n_i + 1}{n+m}
$$

## Laplace
Laplace assumes that you see one outcome of every type. 

Recall example of 6-sides die rolls:

- X ~ Multinomial(p1, p 2, p 3, p 4, p 5, p 6)
- Roll n = 12 times. Result: 3 ones, 2 twos, 0 threes, 3 fours, 1 fives, 3 sixes
  + MLE: $p_1 =3/12, p_2 =2/12, p_3 =0/12, p_4 =3/12, p_5 =1/12, p_6 =3/12$
  + Laplace estimate: $p_i = \frac{n_i+k}{n+k\times m}$
  + Laplace: $p_1 =4/18, p_2 =3/18, p_3 =1/18, p_4 =4/18, p_5 =2/18, p_6 =4/18$
  + No longer have 0 probability of rolling a three!

## Dirichlet

## MAP for Gamma
- $Gamma(\alpha, \beta)$ is a conjugate for Poisson. 
- Mode of gamma: $\frac{\alpha - 1}{\beta}$

- Prior: 
  + $\theta \sim Gamma(\alpha, \beta) = \frac{\beta^{\alpha}x^{\alpha-1}e^{\beta x}}{\gamma(\alpha)}$
  + Saw $\alpha -1$ total imaginary events during $\beta$ prior time periods.
- Experiment: Observe n events during k time periods.
- Posterior: $\theta | \text{n events in k periods}) \sim Gamma(\alpha + n, \beta + k)$
- MAP: $\theta_{MAP} = \frac{a+n-1}{\beta+k}$

## MAP for Poisson
Let $\lambda$ be the average # of successes in a time period. 

1. What does it mean to have a prior of $\theta \sim Gamma(11, 5)$?
  + Observe 10 imaginary events in 5 time periods, i.e., observe at Poisson rate = 2.
2. Perform experiment and see 11 events in next 2 time periods. Given your prior, what is the posterior distribution?
  + $\theta | \text{n events in k periods}) \sim Gamma(\alpha + n, \beta + k) = Gamma(11+11, 5+2) = Gamma(22, 7)$
3. What is $\theta_{MAP}$?
  + $\lambda = \theta_{MAP} = mode(gamma) = \frac{21}{7} = 3$

## ML: Training Data
Training data: assignments all random variables X and Y <br>
Assume IID data:

$$
(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(n)}, y^{(n)}) \\
m = |x^{(i)}|
$$

where each of the n training datapoints has m features and a single output. 

### Notation
- Single instance: $(x^{(i)}, y^{(i)})$ such that $1 \leq i \leq n$
- Feature vector: $x^{(i)}$ such that $1 \leq i \leq n$
- Single feature value: $x_j^{(i)}$, i.e., $x_m^{(2)}$

## Classification
Predict binary outcome based on datapoints.

### Example: Target Movie "Like" Classification
- How can we predict the class label: will the user like life is beautiful?

#### Brute Force Bayes
$$
\hat{y} = argmax P(y|x)
$$

##### m = 1
Return 0 if $P(X_1|y=0)P(y=0) > P(x_1|y=1)P(y=1)$ else 1.

##### m = 2:
\[
\begin{aligned}
\hat{y} &= argmax P(y|x) \\
&= argmax \frac{P(y|x)P(y)}{P(x)} \\
&= argmax P(x|y)P(y) \\
&= argmax P(x_1, x_2|y)P(y)
\end{aligned}
\]

##### Big O of Brute Force Joint
Big O for number of parameters with m = number of features?

$$
O(2^m)
$$

Assuming each feature is binary!

### Naive Bayes
#### Naive Bayes Assumption: 

$$
P(x|y) = P(x_1, x_2, ..., x_m|y) = \prod_i P(x_i|y)
$$

#### Naive Bayes as a Model:
- Class Label: $Y \sim Bern$
- Data distribution: $X_i|Y \sim Bern$
- $P(x,y) = P(y) \prod_i P(x_i|y)$

#### Naive Bayes Classifier
\[
\begin{aligned}
\hat{y} = g(x) &= argmax \ \hat{P}(y|x) \\
&= argmax \ \hat{P}(x|y)\hat{P}(y) \\
&= argmax \ (\prod_i \hat P(x_i|y) \hat P(y)) \\
&= argmax \ log \hat P(y) + \sum_{i=1}^m log \hat P(x_i|y)
\end{aligned}
\]

## Implementing model
### Computing Probabilities from Data {.tabset}
#### MLE
\[
\begin{aligned}
\hat P (Xi = 1|Y = 0) = \frac{\text{(# training examples where Xi = 1 and Y = 0)}}{\text{(# training examples where Y = 0)}}
\end{aligned}
\]

#### MAP
\[
\begin{aligned}
\hat P (Y = 1) = \frac{\text{(# training examples where Y = 1)+1}}{\text{(# training examples)+2}}
\end{aligned}
\]

Training Naive Bayes is estimating parameters for a multinomial (or bernoulli). Thus, training is just counting. 

### Testing data
- Precision = # correctly predicted class Y/ # predicted class Y 
- Recall = # correctly predicted class Y / # real class Y messages

### Gradient Ascent
Logistic regression LL function is convex. 
If someone gives you a gradient package, minimize the negative log likelihood. 
