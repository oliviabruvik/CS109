---
title: 'CS109: Introduction to Probability for Computer Scientists'
author: "Olivia Beyer Bruvik"
date: "Winter 2022"
output:
  pdf_document: default
  html_document: default
subtitle: 'Lecture 16: Beta'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lecture 16: Beta: The RV for Probabilities {.tabset}

## Topics

## Introduction to Beta
The parameter p to a binomial can be a random variable.

### Example: flip a coin with unknown probability {.tabset}
n=9 heads out of n+m=10 flips. Belief in p: $p = \frac{9}{10}$

Let X be our belief about the probability of heads:
$$
P(X=x|H=9,T=1) = \frac{P(H=9,T=1|X=x)f(X=x)}{P(H=9, T=1)} 
= \frac{{10 \choose 9} x^9 (1-x)^1}{P(H=9, T=1)}
= K \times x^9(1-x)^1
$$

#### Frequentist approach (never prior)
$$
X = lim_{n+m \Rightarrow \infty} \frac{n}{n+m} \approx \frac{n}{n+m}
$$

#### Bayesian (with prior)
$$
F_{X|N}(x|n) = \frac{P(N=n|X=x)f_X(x)}{P(N=n)} = 
\frac{{{n+m} \choose n} x^n (1-x)^m}{P(N=n)} =
\frac{{{n+m} \choose n}}{P(N=n)}x^n (1-x)^m
$$

## Beta Derivation
If you start with a X~Uni(0,1) prior over probability, and observe:

- n "successes"
- m "failures"

Your new belief about the probability is:
$$
f_X(x) = \frac{1}{c} \times x^n(1-x)^m \ where \ c = \int_0^1 x^n(1-x)^m
$$

## Beta Random Variable
- A belief distribution over the value of a probability p from a Binomial distribution after observing a-1 successes and b-1 fails.
- X is a Beta Random Variable: $X \sim Beta(a,b)$
- Parameters: 
    + $a = successes + 1$
    + $b = failures + 1$
    
Probability Density Function (PDF): 
\[
\begin{cases}
  \frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1} & 0<x<1 \\
  0 & otherwise
\end{cases}
\ where \ B(a,b) = \int_0^1x^{a-1}(1-x)^{b-1}dx
\]

$$
E[X] = \frac{a}{a+b} \\
Var(X) = \frac{ab}{(a+b)^2(a+b+1)}
$$

## Beta as a conjugate distribution
For Beta, prior and posterior parametric forms are the same: if prior distribution of X is Beta, then the posterior distribution of X is Beta.

$X|N \sim Beta(n+a, m+b)$:
\[
\begin{aligned}
f(X=x|N=n) &= \frac{{{n+m} \choose n }x^n (1-x)^m f(X=x)}{P(N=n)} \\
&= K_1 {{n+m} \choose n}x^n (1-x)^m \frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1} \\
&= K_3 x^{n}(1-x)^{m}x^{a-1}(1-x)^{b-1} \\
&= K_3 x^{n+a-1}(1-x)^{m+b-1}
\end{aligned}
\]

## Beta(a=1,b=1) = Uni(0,1)
$X | (N = n, M = m) \sim Beta(a = n + 1, b = m + 1)$
- Prior $X \sim Uni(0,1)$
- $Beta(a=1,b=1) = \frac{1}{B(a,b)}x^0(1-x)^0 = \frac{1}{\int_0^1 1 dx}1 = 1$
- Prior $X \sim Beta(a=1,b=1)$

## Laplace Smoothing
$$
prior: \ X \sim Beta(a=2, b=2)
$$

## Example: Optimal Decision Making
You try drug B five times. It is successful 2 times. 

- If you had a uniform prior, what is your posterior belief about the likelihood of success:
    + $X \sim Beta(a=3, b=4)$
- What is the expectation of X?
    + $E[X] = \frac{a}{a+b} = \frac{3}{3+4} \approx 0.43$
- What is the probability that X > 0.6?
    + $P(X>0.6) = 1-P(X<0.6) = 1-F_X(0.6) = 1-stats.beta.cdf(0.6, 3, 4) = 0.1792$

