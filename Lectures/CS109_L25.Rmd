---
title: 'CS109: Probability for Computer Scientists'
author: "Olivia Beyer Bruvik"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
  output_dir: "/Users/oliviabeyerbruvik/Documents/Stanford/Y1Q2/CS109/CS109_markdowns/reports"
subtitle: 'Lecture 25: AI and Ethics'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lecture 25: AI and Ethics {.tabset}

## Topics

## Framework of Harm
1. Quality of Service Harms
    + Occur when a system does not work as well for one person as it does for another. 
2. Distributive harms
    + Occur when AI systems extend or withold opportunities, resources, or information. 
3. Existential harms
    + Maybe you will just break the whole damn thing. 

## Detecting Bias {.tabset}

### Biases in ImageNet
- ImageNet is biased towards texture. 
- Classification of the minority group may be worse, even with "awareness" or "stereotyping".

### Problem 1: Undersampling and Lack of Data
- For both gender and race, the majority groups are often undersampled in image databases.
  + Huge improvement in face datasets in 2018 towards a more representative dataset. 
- St. George's admission algorithm docked points from female and "Non-Caucasian" applicants.

1. Garbage in, garbage out. 
2. Improper use of "Sensitive Features"
3. Can be biased without being evil. 

### Formal Fairness
#### Two Philosophic Values of Fairness
1. Procedural fairness: focuses on classification process. 
2. Distributive fairness: focuses on classification outcome.

#### Three Deifinitions of Fairness{.tabset}
##### Fairness through Unawareness
1. Exclude the sensitive feature from dataset
2. Also exclude proxies for the sensitive features
3. Protected groups under EEO are race, color, national origin, religion, age, sex, sexual orientation, physical or mental disability, and reprisal. 

##### Fairness through Awareness: Independence
Parity: an algorithm satisfies parity if the probability that the algorithm makes a positive prediction (G=1) is the same regardless of being conditioned on demographic variable. 
$$P(G=1|D=1) = P(G=1|D=0)$$

##### Fairness through Awareness: Separation
Calibration: the probability that the algorithm is correct (G=T) is the same regardless of demographics. 

$$
P(G=T|D=0) = P(G=T|D=1) \\
\frac{P(G=T|D=1)}{P(G=T|D=0)} \leq 1- \epsilon
$$

## What are you going to do about it?
1. Balanced Training Data
2. Transparent Reporting
3. Train bias out
    + Adversarial learning: predicting "Recidivism" by optimizing model that predicts outcome and model that extracts demographics.
    + Don't use black box algorithms to make recidivism predictions. 
4. Use a Bayes Net
    + Can be better than a black box due to more clarity.

Justice beyond distribution

- Zero-sum: resources and outcomes are fixed
- Leveling Up and Expanding the Pie: outcomes and resources are not fixed

## The Blind Spots
    