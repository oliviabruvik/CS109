---
title: 'CS109: Probability for Computer Scientists'
author: "Olivia Beyer Bruvik"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
  output_dir: "/Users/oliviabeyerbruvik/Documents/Stanford/Y1Q2/CS109/CS109_markdowns/reports"
subtitle: 'Lecture 26: Deep Learning'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lecture 26: Deep Learning {.tabset}

## Topics

## Deep learning
### Definitions
- Deep learning: maximum likelihood estimation with neural networks. Gets its intelligence from its thetas
- Neural networks: multiple logistic regression pieces stacked on top of each other. 

### Digit Recognition Example
Making feature vectors from pictures of numbers: $x^{(i)} = [0,0,0,0,....,1,0,1,....,1,1]
Computer vision: 

- Vision in brain based on hundreds of millions of neurons. 
- Visual neurons make up 30% of your cortex. 
- Computer put logistic regressions on top of each other
    + weigh parameters
    + sum them up
    + input

## Training Neural Networks
1. Understand chain rule as the basis of deep learning
2. Deep learning as MLE

### Notation
$$
layer \ x, x_i \Rightarrow \theta_{i,j}^h \Rightarrow layer \ h, h_j \Rightarrow \theta_j^{\hat y} \Rightarrow layer \ y, \hat y \\
h_j = \sigma(\sum_{i=0}^{m_x}x_i \theta_{i,j}^h) \\
\hat y = \sigma(\sum_{i=0}^{m_h} h_i \theta_{j}^{\hat y}) \\
LL(\theta) = ylog \hat y + (1-y)log[1-\hat y]
$$

- if y = 1, the likelihood is $\hat y$. 
- If y = 0, the likelihood is the PMF of a bernoulli taking on a value of 0 ($1-\hat y$) after claiming that the likelihood of y is $\hat y$. 
- $\hat y = P(y=1|x)$, which is a probability. 
- Go from probability to likelihood by 

-$\theta y$ parameters = $|y|$
- Total parameters = $|x||y| + |y|$

Maximize likelihood of our training data by using optimization techniques:
3. Get partial derivative of log likelihood with respect to each theta
4. Repeatedly apply gradient descent

1. Make deep learning assumption
    + $P(Y=1|X=x) = \hat y$
    + $P(Y=0|X=x) = 1- \hat y$
2. Calculate the log probability of all data
    + $LL(\theta) = \sum_{i=0}^n y^{i}log \hat y^{(i)} + (1-y^{(i)})log[1-\hat y^{(i)}]$
3. Get partial derivative of log likelihood with respect to each theta
    + Chain rule: $\frac{\delta f(z)}{\delta x} = \frac{\delta f(z)}{\delta z} \times \frac{\delta z}{\delta x}$
    + Log likelihood: $\frac{\delta LL(\theta)}{\delta \theta_i^{(\hat y)}} = \frac{\delta LL}{\delta \hat y} \times \frac{\delta \hat y}{\delta \theta_i^{(\hat y)}}$
    + $\frac{\delta LL(\theta)}{\delta \theta_{i,j}^{(h)}} = \frac{\delta LL}{\delta \hat y} \times \frac{\delta \hat y}{\delta \theta_i^{(\hat y)}}$
    + 
