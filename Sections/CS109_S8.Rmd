---
title: "CS109: Introduction to Probability for Computer Scientists"
subtitle: "Section 8: Machine Learning"
author: "Olivia Beyer Bruvik"
date: "Winter 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Section 8: Machine Learning {.tabset}

## Summary {.tabset}
Best model to describe the data
1. Which RV to use?
  - 0 or 1: Bernoulli --> p
  - p of p: Beta --> a, b
2. Parameter of RV 

### MLE
$$
\hat \theta_{max} = P(D|\theta) \\
L(\theta) = \prod f(X_i|\theta) \\
LL(\theta) = \sum log \ f(X_i| \theta) \\
\frac{dLL(\theta)}{dt} = 0
$$

### MAP
\[
\begin{aligned}
argmax \ \hat \theta_{max} = argmax \ P(\theta | D) \\
= argmax \ \frac{P(\theta | D)P(\theta)}{P(D)} \\
= argmax \ P(D | \theta)P(\theta)
\end{aligned}
\]

## Problem 1: MAP {.tabset}
### Problem: Why Boba Cares About MAP
You donâ€™t understand why thereâ€™s no boba place within walking distance around campus, so you decide to start one. In order to estimate the amount of ingredients needed and the time you will spend in the business (you still need to study), you want to estimate how many orders you will receive per hour. After taking CS109, you are pretty confident that incoming orders can be considered as independent events and the process can be modeled with a Poisson.
<br>
Now the question is - what is the ğœ† parameter of the Poisson? In the first hour of your soft opening, you are visited by 4 curious students, each of whom made an order. You have a prior belief that $f(\delta = \lambda) = K \times \lambda \times e^{\frac{- \lambda}{2}}$. What is the MLE estimate? What is inference of ğœ† given the observation? What is the Maximum-A-Posteriori (MAP) estimate of ğœ†? Through your process try to identify what is a point-estimate, and what is a distribution.

### Solution {.tabset}
- M = Poisson
- The $\lambda$ parameter of the Poisson is beta(a, b). <br>
- Observed data, D: $X_0 = 4$ students/hour.
- Prior belief: $f(\delta = \lambda) = K \times \lambda \times e^{\frac{- \lambda}{2}}$

#### MLE estimate
\[
\begin{aligned}
L(\theta) &= \prod f(X_i|\theta) \\
&= f(X_0 | \theta) \\
&= \frac{\lambda^4 e^{-\lambda}}{4!} \\
\\
\hat \lambda &= argmax \ L(\lambda) \\
&= argmax \ \frac{\lambda^4 e^{-\lambda}}{4!} \\
&= argmax \ \lambda^4 e^{-\lambda} \\
&= argmax \ log(\lambda^4 e^{-\lambda}) \\
&= argmax \ log(\lambda^4) + log(e^{-\lambda}) \\
&= argmax \ 4log(\lambda) -\lambda \\
\\
\frac{dLL}{d \lambda} &= \frac{4}{\lambda} - 1 = 0 \Rightarrow \frac{4}{\hat \lambda} - 1 = 0 \Rightarrow \lambda = 4
\end{aligned}
\]

#### MAP estimate
\[
\begin{aligned}
f(\lambda | X = 4) &= \frac{f(X-4 | \lambda)f(\lambda)}{P(X=4)} \\
\hat \theta_{map} &= argmax \ \frac{f(X-4 | \lambda)f(\lambda)}{P(X=4)} \\
&= argmax \ f(X-4 | \lambda)f(\lambda) \\
&= argmax \ \frac{\lambda^4 e^{-\lambda}}{4!} \times K \times \lambda \times e^{\frac{- \lambda}{2}} \\
&= \lambda^5 e^{-\frac{3\lambda}{2}} \\
\\
\frac{dLL}{d \lambda} &= \frac{5}{\lambda} - \frac{3}{2} = 0 \Rightarrow \frac{5}{\hat \lambda} - \frac{3}{2} = 0 \Rightarrow \hat \lambda_{map} = \frac{10}{3}
\end{aligned}
\]

## Problem 2: MLE {.tabset}
### Problem: Vision Test MLE
You decide that the vision tests given by eye doctors would be more precise if we used an approach inspired by logistic regression. In a vision test a user looks at a letter with a particular font size and either correctly guesses the letter or incorrectly guesses the letter.

You assume that the probability that a particular patient is able to guess a letter correctly is: 
$$
p = \sigma(\theta + f)
$$

Where ğœƒ is the userâ€™s vision score and ğ‘“ is the font size of the letter. This formula uses the sigmoid function:

$$
\sigma(z) = \frac{1}{1+e^{-z}} \\
\frac{\delta \sigma(z)}{\delta z} = \sigma(z)[1-\sigma(z)]
$$

Explain how you could estimate a userâ€™s vision score (ğœƒ) based on their 20 responses $(f^{(1)}, y^{(1)}), ..., (f^{(20)}, y^{(20)})$, where $y^{(i)}$ is an indicator variable for whether the user correctly identified the ğ‘–th letter and $f^{(i)}$ is the font size of the ğ‘–th letter. Solve for any and all partial
derivatives required by the approach you describe in your answer.

### Solution
- Observed data, D: $(f^{(1)}, y^{(1)}), ..., (f^{(20)}, y^{(20)})$
- : Bernoulli (output can take values 0 and 1)
- Prior belief: $f(\delta = \lambda) = K \times \lambda \times e^{\frac{- \lambda}{2}}$

\[
\begin{aligned}
p = \sigma (\theta + f)
LL(\theta) &= \prod_{i=1}^{20} p^{1-y_1}(1-p)^{1-y_i} \\
&= \sum_{i=1}^{20} [y_1log(p) + (1-y_i)log(1-p)] \\
\\
\frac{\delta LL(\theta)}{\delta (\theta)} &= \frac{\delta LL(\theta)}{\delta p} \times \frac{\delta p}{\delta \theta} \\
&= (\frac{y_i}{p} - \frac{1-y_i}{1-p})p(1-p) \\
&= y_i(1-p) - p(1-y_i) \\
&= y_i - p = y_i - \sigma(\theta +f) \\
&= \sum_{i=1}^20 [y_i-\sigma(\theta + f^{(i)})] \\
\end{aligned}
\]

Next, gradient ascent to optimize. Find the value of $\theta$ where $\frac{\delta LL(\theta)}{\delta (\theta)} = 0$. 
